I. Access Layer Design
Due to the loss of original code and documentation, the following outlines the core architectural concepts:

Frontend Access:
A stateless unified API entry is built with OpenResty and Lua, achieving high concurrency, low coupling, and abstraction. All requests are routed through a single gateway and undergo unified encryption.

Access Layer Encryption Process:

Parameter Encapsulation: Core parameters are formatted as JSON, directly mapping to MongoDB collection structures, including the table name and corresponding business data.

Custom Base64-like Obfuscation:

Characters are first rearranged according to a predefined rule.

Using a custom character-to-digit mapping table, each character is converted to a digit and then expanded into an 8-bit binary representation.

The entire binary string is regrouped into 6-bit chunks, each converted back to a digit, and then mapped to a character via the same table, resulting in an unreadable obfuscated string.

Integrity Verification: The obfuscated parameters are concatenated with a timestamp and a secret key using a defined rule, and an MD5 hash is generated to produce a signature for tamper-proof validation.

Anti-abuse Mechanism: Redis is employed to enforce rate limiting and filter duplicate requests.

High-Performance Processing: The access layer only performs decryption, Redis-based deduplication, and Kafka message writing, with no complex business logic. A single node can achieve tens of thousands of QPS.

Internal Service Invocation:
For different business lines within the internal production network, multi-language SDKs (e.g., Go, Java) are provided to encapsulate Kafka interaction details. Each business line only needs to write data into Kafka according to the agreed specification, decoupling data collection from business logic.

Design Advantages:

High decoupling is achieved through standardized database schemas (MongoDB) and messaging protocols (Kafka).

Adding a new business requires only two modifications: frontend instrumentation and an extension in the statistics module, typically taking less than five minutes, with no changes needed in other layers.

II. Data Persistence
A Kafka consumer developed in Go continuously pulls messages from the access layer, parses them, and stores the data into MongoDB. Data is sharded by date to support massive write throughput and efficient querying.

III. Data Statistics (dataStatics Module)
Offline Batch Processing: Python scripts run daily in the early morning to aggregate incremental data in MongoDB (query load is manageable due to date-based sharding).

Real-time Stream Processing: Spark Streaming consumes real-time data from Kafka to compute live metrics, which are then cross-validated with offline batch results to ensure data consistency.

Data Lifecycle Management: Includes migration of legacy business data, separation of hot and cold data, and archiving strategies to optimize storage and query performance.

IV. Data Visualization
As the visualization layer is intended solely for internal use, the frontend queries the aggregated results stored in MongoDB (e.g., daily summary reports, pre-computed metrics) through a unified API interface. These statistical results are generated by offline batch or real-time stream processing modules and persisted in MongoDB. The backend API maintains the same stateless design as the access layer, merely forwarding query requests without complex business logic, thus ensuring lightweight efficiency.

----------------------------------------------------------------------------------------------------------------------
一、接入层设计
由于历史代码与文档缺失，以下为架构核心思路概述：

前端接入：
采用 OpenResty + Lua 构建统一的无状态 API 入口，实现高并发、低耦合与抽象封装。所有请求均通过单一网关，并统一进行加密处理。

接入层加密流程：

参数封装：核心参数采用 JSON 格式，直接映射 MongoDB 的集合结构，包含表名及对应业务数据。

自定义类 Base64 混淆编码：

按预设规则对原始字符进行重排；

依据自定义字符‑数字映射表，将每个字符转换为对应数字，再扩展为 8 位二进制；

将整体二进制串按 6 位重新分组，每组转换为数字后，通过映射表还原为字符，最终生成不可读的乱码串。

完整性校验：将上述编码后的参数与时间戳、密钥按约定规则拼接，经 MD5 哈希生成签名，用于参数防篡改校验。

防刷机制：基于 Redis 实现请求频率控制与重复请求过滤。

高性能处理：接入层仅执行解密、Redis 防重校验及消息写入 Kafka，无复杂业务逻辑，单机并发能力可达万级 QPS。

内部服务调用：
面向生产网内部不同业务线，提供多语言 SDK（如 Go、Java 等），封装与 Kafka 的交互细节。各业务线只需按约定规范将数据写入 Kafka 即可，实现埋点采集与业务逻辑的解耦。

设计优势：

通过标准化数据库结构（MongoDB）与消息协议（Kafka），实现各模块间的高度解耦。

新增业务仅需两处改动：前端埋点添加与统计模块代码扩展，平均耗时不超过 5 分钟，其余层无需变更。

二、数据持久化
使用 Go 语言开发 Kafka 消费者，实时拉取接入层写入的消息，经解析后存入 MongoDB。数据按日期分片存储，以支撑海量写入与高效查询。

三、数据统计（dataStatics 模块）
离线批量计算：每日凌晨利用 Python 脚本对 MongoDB 中的日增量数据进行聚合统计（得益于日期分片，查询压力可控）。

实时流计算：采用 Spark Streaming 处理 Kafka 实时数据流，产出实时指标，并与离线统计结果进行交叉验证，确保数据一致性。

数据生命周期管理：包含历史业务数据迁移、冷热数据分离与归档策略，优化存储与查询性能。

四、数据展示
由于展示端仅限内部使用，前端通过统一的 API 接口直接查询 MongoDB 中存放的统计结果数据（例如按日汇总的报表、预计算指标等）。这些统计结果由离线或实时统计模块生成并存入 MongoDB。后端接口延续接入层的无状态设计，仅做查询转发，不包含复杂业务逻辑，确保轻量高效。





