一、接入层：  
由于代码和文档暂时找不到了，所以将实现的大概思路表达一下：
前端部分：
openresty+lua, 接口统一只有一个入口，无状态，高解耦，抽象，加密等；
接入层加密：
1、核心参数： json格式，参数直接面象mongodb collection对象，传入表名：具体的相关数据等；
2、对核心参数进行类似base64加密：先是按照某种规则对字符进行打乱，然后接接，
按照自己设定的字符和数字对应表，生成相对应的一个个数字数组，再将每一个数字生成8位二进制，
然后将整体拼接起来，重新按6位来分成一个个二进制数，再生成数字按照同样的字符数字对应表找到相对应的字符，
再将一个个字符按顺序接接输出； 所以用户看到的是一堆乱码；
3、再将刚刚用类base64算法生成的参数，配合时间戳，密钥,将这三个参数按照某种规则拼接起来，然后md5加密，做参数的一致性验证；
4、利用redis做防刷；
5、接口只做解密，rediss查重刷，以及写入kafka的动作，所以单机并发上万不是难题，

内部应用部分：（生产线内部网络，不同的环境和生产线（同一部门，但不同业务），需要做埋线）：
根据不同的语言，写了两种sdk，让他们直接下载使用即可； （直接利用kafka通信，按照约定的规范写入kafka即可）

结论：
1、这种实现方式：只要定好数据库存储规则，kafka写入规则即可；  
2、有新的业务需求，只需要加两点，一个是前端埋点，第二个是统计部分加一个统计代码即可，不超过5分钟； 其他层面都不需要更改；

二、数据存储： （）
go 消费 kafka，将kafka的数据存入mongodb;

三、数据统计： （dataStatics文件夹）
1、python凌晨离线统计（mongodb按日期分片，无压力），
2、spark-streaming做实时统计， 与离线统计会进行对账；
3、数据迁移（旧有业务），冷热数据处理；

四、数据展示：  
这个也非常简单，由于数据展示仅供内部使用，由前端开发，这边后端接口也是统一入口，直接查询mongodb(统计好的数据也会存入到mongodb中)；

